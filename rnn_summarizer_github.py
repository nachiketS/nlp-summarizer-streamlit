# -*- coding: utf-8 -*-
"""RNN Summarizer - Github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18d1RGgmEibi__qo3DllgvXHu5-2nSQYl

## Imports
"""

import numpy as np
import pandas as pd
import re
import string
import matplotlib.pyplot as plt
from tqdm import tqdm, trange
import os


import torch
import torch.nn as nn
import torch.optim as optim
import torchtext as torchtext

# from torchtext.legacy.data import Field, BucketIterator, Example, Dataset, Iterator
# from torch.utils.data import DataLoader, random_split

import spacy
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.porter import PorterStemmer

import random
import math
import time

import torch.nn.functional as F

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import sys
from collections import Counter
import random

SOS_token = 0
EOS_token = 1
UNK_token = -1
class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: "SOS", 1: "EOS", 2: "UNK"}
        self.n_words = 3  # Count SOS and EOS

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

def readLangs(text, summary, reverse=False):
    print("Reading lines...")
    
    # Split every line into pairs and normalize
    pairs = [[text[i], summary[i]] for i in range(len(text))]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(summary)
        output_lang = Lang(text)
    else:
        input_lang = Lang(text)
        output_lang = Lang(summary)

    return input_lang, output_lang, pairs

def prepareData(text, summary, reverse=False):
    input_lang, output_lang, pairs = readLangs(text, summary, reverse)
    print("Read %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    # print("Counted words:")
    # print(input_lang.name, input_lang.n_words)
    # print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs

# data = pd.read_csv("gdrive/MyDrive/6120project/dataset_750/train.csv" ,encoding="utf-8")
data = pd.read_csv("dataset_750/train.csv" ,encoding="utf-8")

nltk.download('stopwords')

UNK = "<UNK>"
SENT_BEGIN = "SOS"
SENT_END = "EOS"

hyperlinks_str = r'\bhttp[sS]?://[A-Za-z0-9\./-_$&@=\+%#\(\)!`~\?\]\[\}\{<>\^\*]+\b'
unicode_str = r'&[A-Za-z0-9]+;'
punct_str = r'[^A-Za-z0-9</> ]'
whitespace_str = r'\s+'

def string_cleanup(string_):
    #replacing hyperlinks to http, most of the spams can have a hyperlink
    str_ = re.sub(hyperlinks_str,'http',string_) 
    #remove unicode 
    str_ = re.sub(unicode_str,'',str_)
    #remove puctuations 
    str_ = re.sub(punct_str,'',str_)
    #remove extra whitespace  
    str_ = re.sub(whitespace_str,' ',str_)
    return str_.strip().lower()

stop_words = set(stopwords.words('english'))
stop_words = stop_words.union(set(['may','might','can','could','have','had','has']))
def clean_message(message, stopword_ = True):
    '''
    Input:
        message: a string containing a message.
    Output:
        messages_cleaned: a list of words containing the processed message. 

    '''
    #stemmer = porter_stemmer
    english_stopwords = stop_words
    if stopword_ == False:
        english_stopwords = []
    message = string_cleanup(message)
    message_tokens = message.split()

    messages_cleaned = []
    last_word = SENT_BEGIN
    for word in message_tokens:
        if word not in english_stopwords:  #removing stopwords
            #stem_word = stemmer.stem(word)  #using stemming
            if last_word==SENT_BEGIN and word == SENT_END:
                messages_cleaned = messages_cleaned[:-1]
            else:
                last_word = word
                messages_cleaned.append(word)       

    return messages_cleaned

def clean_message_to_string(input):
  return " ".join(clean_message(input))

Articles = []
Summaries = []

for text in data["src_txt"].tolist():
  Articles.append(clean_message_to_string(text))

for target in data["tgt_txt"].tolist():
  Summaries.append(clean_message_to_string(target))

input_lang, output_lang, pairs = prepareData(Articles, Summaries , False)
# print(random.choice(pairs))

word_counter = Counter(input_lang.word2count)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# vectors = GloVe(name='6B', dim='100')
# v = vocab.Vocab(input_land.word2count, min_freq=3, specials=['<unk>', 'SOS', 'EOS'], vectors=vectors)

vocab = torchtext.vocab.vocab(word_counter)
# .build_vocab_from_iterator(input_lang.word2count, min_freq=2, specials=["SOS", "EOS"])

pretrained_vectors = torchtext.vocab.GloVe(name='6B', dim=300)

pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(vocab.get_itos())

class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding ,freeze=True) 
        self.gru = nn.GRU(300, hidden_size) #Hardcoding, since fasttext is 300

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output, hidden = self.gru(embedded, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device = device)

class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(300, hidden_size) #Hardcoding , since fasttext gives out 300 
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device = device)

MAX_LENGTH = 750

def indexesFromSentence(lang, sentence):
    return [lang.word2index[word] for word in sentence.split(' ')]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

teacher_forcing_ratio = 0.5
def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden

    if random.random() < teacher_forcing_ratio:
      use_teacher_forcing = True 
    else:
      use_teacher_forcing = False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)#, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)#, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

# import time
# import math


# def asMinutes(s):
#     m = math.floor(s / 60)
#     s -= m * 60
#     return '%dm %ds' % (m, s)


# def timeSince(since, percent):
#     now = time.time()
#     s = now - since
#     es = s / (percent)
#     rs = es - s
#     return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

# import matplotlib.pyplot as plt
# plt.switch_backend('agg')
# import matplotlib.ticker as ticker
# import numpy as np


# def showPlot(points):
#     plt.figure()
#     fig, ax = plt.subplots()
#     # this locator puts ticks at regular intervals
#     loc = ticker.MultipleLocator(base=0.2)
#     ax.yaxis.set_major_locator(loc)
#     plt.plot(points)

# def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
#     print("Training....")

#     loss_list = []

#     start = time.time()
#     plot_losses = []
#     print_loss_total = 0  # Reset every print_every
#     plot_loss_total = 0  # Reset every plot_every

#     encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
#     decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
#     training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]
#     criterion = nn.NLLLoss()

#     for iter in range(1, n_iters + 1):
#         if iter% 1000 == 0:
#             print(iter,"/",n_iters + 1)
#         training_pair = training_pairs[iter - 1]
#         input_tensor = training_pair[0]
#         target_tensor = training_pair[1]

#         loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)
#         print_loss_total += loss
#         plot_loss_total += loss

#         if iter % print_every == 0:
#             print_loss_avg = print_loss_total / print_every
#             print_loss_total = 0
#             loss_list.append(print_loss_avg)
#             print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))

#         if iter % plot_every == 0:
#             plot_loss_avg = plot_loss_total / plot_every
#             plot_losses.append(plot_loss_avg)
#             plot_loss_total = 0

#     return showPlot(plot_losses), loss_list

def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS
    
        decoder_hidden = encoder_hidden

        decoded_words = []

        for di in range(max_length):
            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
            topv, topi = decoder_output.data.topk(1)

            if topi.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words

def evaluateRandomly(encoder, decoder, n=10):
    for i in range(1):
        pair = random.choice(pairs)
        print('>', pair[0])
        print('=', pair[1])
        output_words = evaluate(encoder, decoder, pair[0])
        output_sentence = ' '.join(output_words)
        print('<', output_sentence)
        print('')

hidden_size = 300
encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)

# encoder1

decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)

# decoder1

hidden_size = 300
# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)
# decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)
iterations = 30000
# loss_list = trainIters(encoder1, decoder1, iterations, print_every=iterations//150)

# val_data = pd.read_csv("gdrive/MyDrive/6120project/dataset_750/val.csv" ,encoding="utf-8")
# val_Articles = []
# val_Summaries = []

# for text in val_data["src_txt"].tolist():
#   val_Articles.append(clean_message_to_string(text))

# for target in val_data["tgt_txt"].tolist():
#   val_Summaries.append(clean_message_to_string(target))

# val_input_lang, val_output_lang, val_pairs = prepareData(val_Articles, val_Summaries , False)

# def evaluateValPairs(encoder, decoder):
#     for pair in val_pairs:
#       print('>', pair[0])
#       print('=', pair[1])
#       output_words = val_evaluate(encoder, decoder, pair[0])
#       output_sentence = ' '.join(output_words)
#       print('<', output_sentence)
#       print('')
    
    
#     for i in range(1):
#         pair = random.choice(pairs)

# def val_evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
#     with torch.no_grad():
#         input_tensor = tensorFromSentence(val_input_lang, sentence)
#         input_length = input_tensor.size()[0]
#         encoder_hidden = encoder.initHidden()

#         encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

#         for ei in range(input_length):
#             encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
#             encoder_outputs[ei] += encoder_output[0, 0]

#         decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS
    
#         decoder_hidden = encoder_hidden

#         decoded_words = []

#         for di in range(max_length):
#             decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
#             topv, topi = decoder_output.data.topk(1)

#             if topi.item() == EOS_token:
#                 break
#             else:
#                 decoded_words.append(val_output_lang.index2word[topi.item()])

#             decoder_input = topi.squeeze().detach()

#         return decoded_words

# decoderpath = 'rnn_models/decoder.pt'
# encoderpath = 'rnn_models/encoder.pt'
# encoder1 = torch.load(encoderpath, map_location=torch.device('cpu'))
# decoder1 = torch.load(decoderpath, map_location=torch.device('cpu'))


UNK_token = -1

def prod_indexesFromSentence(lang, sentence):
    to_ret = []
    for word in sentence.split(' '):
        if word not in lang.word2index:
            to_ret.append(UNK_token)
        else:
            to_ret.append(lang.word2index[word])
    return to_ret


def prod_tensorFromSentence(lang, sentence):
    indexes = prod_indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)

def prod_evaluate(sentence, encoder = encoder1, decoder = decoder1, lang = input_lang, max_length = 750):
    with torch.no_grad():
        input_tensor = prod_tensorFromSentence(lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden

        decoded_words = []

        for di in range(max_length):
            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
            topv, topi = decoder_output.data.topk(1)

            if topi.item() == EOS_token:
                break
            else:
                decoded_words.append(lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return " ".join(decoded_words)

# evaluated_pairs = evaluateValPairs(encoder1, decoder1)

# evaluateRandomly(encoder1, decoder1)

# generated = []
# !pip install Rouge
# for sent in val_Articles:
#   # print(sent)
#   output_words = val_evaluate(encoder1, decoder1, sent)
#   output_sentence = ' '.join(output_words)
#   generated.append(output_sentence)

# !pip install Rouge

# from rouge import Rouge
# rouge = Rouge()
# scores = rouge.get_scores(generated, val_Summaries,avg=True)

# scores

# generated

